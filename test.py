import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
import time
from lstm_model import LSTMModel
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense


# Load the CSV data
df = pd.read_csv('dataset.csv')

# Data Filtering
df = df[df['Length'] != 0]
max_length_per_file = df.groupby('File')['Length'].max()
quantile_99_max_per_file = max_length_per_file.quantile(0.99)
filtered_df = df[~df['File'].isin(max_length_per_file[max_length_per_file > quantile_99_max_per_file].index)]

# Group by 'File' and keep the first label
grouped_df = filtered_df.groupby('File', as_index=False).first()

# Split based on grouped data
train_files_grouped, test_files_grouped = train_test_split(grouped_df, test_size=0.3, random_state=42, stratify=grouped_df['Label'])

# Extract the 'File' column values
train_files = train_files_grouped['File']
test_files = test_files_grouped['File']

# Further split filtered_df based on 'File' column values
train_data = filtered_df[filtered_df['File'].isin(train_files)]
test_data = filtered_df[filtered_df['File'].isin(test_files)]

print("Number of unique files in dataframe:", grouped_df['File'].nunique())
print("Number of samples in dataframe:", len(filtered_df))
print("Number of train files:", len(train_files))
print("Number of samples in train_data:", len(train_data))
print("Number of total test files:", len(test_files))
print("Number of samples in test_data:", len(test_data))

# Train data split
test_files_split_grouped, test_functions_grouped = train_test_split(test_files_grouped, test_size=0.5, random_state=42, stratify=test_files_grouped['Label'])

# Extract the 'File' column values
test_files_split = test_files_split_grouped['File']
test_functions = test_functions_grouped['File']

# Further split filtered_df based on 'File' column values
test_files_data = filtered_df[filtered_df['File'].isin(test_files_split)]
test_functions_data = filtered_df[filtered_df['File'].isin(test_functions)]

print("Number of test files:", len(test_files_split))
print("Number of samples in test_files_data:", len(test_files_data))
print("Number of test functions:", len(test_functions))
print("Number of samples in test_functions_data:", len(test_functions_data))

# Data Preprocessing
sequences = train_data['Sequence'].tolist()
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sequences)

train_sequences_tokenized = tokenizer.texts_to_sequences(train_data['Sequence'])
test_sequences_tokenized = tokenizer.texts_to_sequences(test_data['Sequence'])

max_sequence_length = max(len(seq) for seq in train_sequences_tokenized + test_sequences_tokenized)
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')

# Convert to numpy arrays
X_train = np.array(train_sequences_padded)
y_train = np.array(train_data['Label'])
X_test = np.array(test_sequences_padded)
y_test = np.array(test_data['Label'])

# Reshape input to include the batch size dimension
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # (num_samples, sequence_length, num_features)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Create tf.data.Dataset from numpy arrays
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))

# Shuffle and batch the datasets
batch_size = 128
train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)
test_dataset = test_dataset.batch(batch_size)

# Build the LSTM model
model = Sequential()
model.add(LSTM(32, input_shape=(max_sequence_length, 1), activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model using tf.data.Dataset
model.fit(train_dataset, epochs=10, verbose=1)

# Evaluate the model using tf.data.Dataset
test_loss, test_accuracy = model.evaluate(test_dataset)
print("Test loss:", test_loss)
print("Test accuracy:", test_accuracy)

# Model training
#model = LSTMModel(depth=3, bidirectional=False, num_neurons=320, dropout_rate=0.3, weight_update_algorithm='adam')
#model.compile_model()

# Train the model on the dataset
#history = model.fit(train_dataset, epochs=100, batch_size=batch_size)

# Evaluation and Testing
#test_loss, test_accuracy = model.evaluate(test_dataset)

#end_time = time.time()
#training_and_evaluation_duration = end_time - start_time

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Training and Evaluation Duration (seconds):", training_and_evaluation_duration)
