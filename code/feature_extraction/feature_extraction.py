import os
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from gensim.models import Word2Vec


# Load the CSV data
dataset_directory = os.path.join(os.path.dirname(__file__), '../disassembly/dataset.csv')
df = pd.read_csv(dataset_directory)

# Data Filtering
df = df[df['Length'] != 0]

grouped_df = df.groupby('File').agg({
    'Sequence': ' '.join,
    'Label': 'first'
}).reset_index()

#grouped_df, _ = train_test_split(grouped_df, test_size=0.997, random_state=42, stratify=grouped_df['Label'])

# Further split filtered_df based on 'File' column values
train_data, test_data = train_test_split(grouped_df, test_size=0.3, random_state=42, stratify=grouped_df['Label'])
test_data, validation_data = train_test_split(test_data, test_size=0.3, random_state=42, stratify=test_data['Label'])

print("Number of samples in dataframe:", len(grouped_df))
print("Number of samples in train_data:", len(train_data))
print("Number of samples in validation_data:", len(validation_data))
print("Number of samples in test_data:", len(test_data))

# Load the word embedding
embedding_directory = os.path.join(os.path.dirname(__file__), '../word_embedding_training/word2vec.model')
embedding = Word2Vec.load(embedding_directory)


def compute_embeddings(sequence, embedding_model):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(sequence.split())
    word_counts = tokenizer.word_counts
    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    top_frequent_words = [word_count[0] for word_count in sorted_word_counts[:600]]
    embeddings = [embedding_model.wv[word] for word in top_frequent_words]
    return embeddings


def compute_test_embeddings(sequence, embedding_model):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(sequence.split())
    word_counts = tokenizer.word_counts
    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    top_frequent_words = [word_count[0] for word_count in sorted_word_counts[:600]]
    embeddings = [embedding_model.wv[word] for word in top_frequent_words if word in embedding_model.wv]
    return embeddings


# Generate the feature vectors
train_data['Embedding'] = train_data['Sequence'].apply(lambda x: compute_embeddings(x, embedding))
validation_data['Embedding'] = validation_data['Sequence'].apply(lambda x: compute_embeddings(x, embedding))
test_data['Embedding'] = test_data['Sequence'].apply(lambda x: compute_test_embeddings(x, embedding))

# Save the feature vectors
train_data.to_pickle("precomputed_embeddings_train_data.plk")
validation_data.to_pickle("precomputed_embeddings_validation_data.plk")
test_data.to_pickle("precomputed_embeddings_test_data.plk")
