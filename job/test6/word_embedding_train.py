import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
from tensorflow.keras.preprocessing.text import Tokenizer
from gensim.models import Word2Vec


# Load the CSV data
df = pd.read_csv('../../dataset.csv')

# Data Filtering
df = df[df['Length'] != 0]

grouped_df = df.groupby('File').agg({
    'Sequence': ' '.join,
    'Label': 'first'
}).reset_index()

label_counts = grouped_df['Label'].value_counts()
print(f"The dataset includes {label_counts[0]} benign files and {label_counts[1]} malicious files.")

#grouped_df, _ = train_test_split(grouped_df, test_size=0.997, random_state=42, stratify=grouped_df['Label'])

# Further split filtered_df based on 'File' column values
train_data, test_data = train_test_split(grouped_df, test_size=0.3, random_state=42, stratify=grouped_df['Label'])
test_data, validation_data = train_test_split(test_data, test_size=0.3, random_state=42, stratify=test_data['Label'])

print("Number of samples in dataframe:", len(grouped_df))
print("Number of samples in train_data:", len(train_data))
print("Number of samples in validation_data:", len(validation_data))
print("Number of samples in test_data:", len(test_data))

# Data Preprocessing
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data['Sequence'])
tokenizer.fit_on_texts(validation_data['Sequence'])
train_word_sequences = tokenizer.sequences_to_texts(tokenizer.texts_to_sequences(train_data['Sequence']))
train_word_sequences = [sequence.split() for sequence in train_word_sequences]
validation_word_sequences = tokenizer.sequences_to_texts(tokenizer.texts_to_sequences(validation_data['Sequence']))
validation_word_sequences = [sequence.split() for sequence in validation_word_sequences]

embedding = Word2Vec(sentences=train_word_sequences + validation_word_sequences, vector_size=300, min_count=0, window=100, sg=1)
embedding.save("word2vec.model")
