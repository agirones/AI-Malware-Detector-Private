import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
from lstm_model import LSTMModel
import optuna
import joblib
from tensorflow.keras.callbacks import EarlyStopping



train_data = pd.read_pickle("../test6/precomputed_embeddings_train_data.plk")
test_data = pd.read_pickle("../test6/precomputed_embeddings_validation_data.plk")
validation_data = pd.read_pickle("../test6/precomputed_embeddings_test_data.plk")

print("Number of samples in dataframe:", len(train_data) + len(validation_data) + len(test_data))
print("Number of samples in train_data:", len(train_data))
print("Number of samples in validation_data:", len(validation_data))
print("Number of samples in test_data:", len(test_data))


def train_generator():
    for embedding, label in zip(train_data['Embedding'], train_data['Label']):
        yield embedding, label

def validation_generator():
    for embedding, label in zip(validation_data['Embedding'], validation_data['Label']):
        yield embedding, label

def test_generator():
    for embedding, label in zip(test_data['Embedding'], test_data['Label']):
        yield embedding, label

# Create tf.data.Dataset from RaggedTensors
train_dataset = tf.data.Dataset.from_generator(
        train_generator,
        output_signature=(
            tf.TensorSpec(shape=(None,300), dtype=tf.float16),
            tf.TensorSpec(shape=(), dtype=tf.float16)
            )
    )

validation_dataset = tf.data.Dataset.from_generator(
        validation_generator,
        output_signature=(
            tf.TensorSpec(shape=(None,300), dtype=tf.float16),
            tf.TensorSpec(shape=(), dtype=tf.float16)
            )
    )

test_dataset = tf.data.Dataset.from_generator(
        test_generator,
        output_signature=(
            tf.TensorSpec(shape=(None,300), dtype=tf.float16),
            tf.TensorSpec(shape=(), dtype=tf.float16)
            )
    )

# Shuffle and batch the datasets
train_dataset = train_dataset.shuffle(128).padded_batch(64)
validation_dataset = validation_dataset.shuffle(128).padded_batch(64)
test_dataset = test_dataset.padded_batch(64)

def objective(trial):
    epochs = trial.suggest_int('epochs', 1, 100)
    batch_size = trial.suggest_int('batch_size', 1, 128)
    depth = trial.suggest_int('depth', 1, 3)
    bidirectional = trial.suggest_categorical('bidirectional', [True, False])
    num_neurons = trial.suggest_int('num_neurons', 1, 320)
    drop_out = trial.suggest_categorical('drop_out', [0, 0.1, 0.2, 0.3, 0.4, 0.5])

    # Build the LSTM model
    model = LSTMModel(depth=depth, bidirectional=bidirectional, num_neurons=num_neurons, dropout_rate=drop_out, weight_update_algorithm='adam')

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)

    # Train the model using tf.data.Dataset
    model.fit(train_dataset, epochs=epochs, batch_size=batch_size, verbose=0, validation_data=validation_dataset, callbacks=[early_stopping])

    _, accuracy = model.evaluate(validation_dataset, verbose=0)
    return accuracy

study = optuna.create_study(direction='maximize',
                            pruner=optuna.pruners.HyperbandPruner(
                                min_resource=1, reduction_factor=3
                            )
                       )

study.optimize(objective, n_trials=150)

joblib.dump(study, 'es_optuna_hb/study.pkl')

trial_dataframe = study.trials_dataframe()
trial_dataframe.to_csv('es_optuna_hb/study_trials.csv', index=False)

best_params = study.best_params
print("Optimal Hyperparameters:")
print(best_params)

model = LSTMModel(depth=best_params['depth'], bidirectional=best_params['bidirectional'], num_neurons=best_params['num_neurons'], dropout_rate=best_params['drop_out'], weight_update_algorithm='adam')
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model using tf.data.Dataset
history = model.fit(train_dataset, epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_data=validation_dataset, verbose=2)

model.save('es_optuna_hb/model')
np.save('es_optuna_hb/optimized_params-history.npy', history.history)

# Plot training and validation loss
# Increase the figure size for better visibility
plt.figure(figsize=(14, 7))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.suptitle("Training and Validation Metrics over Epochs", y=0.95)
plt.subplots_adjust(wspace=0.3)

plt.savefig('es_optuna_hb/optimized_params-validation_graph.png', dpi=1200)

# Evaluate the model using tf.data.Dataset
y_pred_probs = model.predict(test_dataset, verbose=2)
y_pred = (y_pred_probs > 0.5).astype(int)
y_true = test_data['Label'].to_list()

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

fpr, tpr, thresholds = roc_curve(y_true, y_pred_probs)
roc_auc = roc_auc_score(y_true, y_pred_probs)

# Print metrics
print()
print("Testing of the model with the optimized parameters:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("ROC AUC:", roc_auc)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')

plt.savefig('es_optuna_hb/optimized_params-roc_curve.png', dpi=1200)

# Plot optimization history and save
history_plot = optuna.visualization.plot_optimization_history(study)
history_plot.write_image('es_optuna_hb/optimization_history.png', scale=6, width=1080, height=1080)

# Plot parameter importances and save
param_importances_plot = optuna.visualization.plot_param_importances(study)
param_importances_plot.write_image('es_optuna_hb/parameter_importances.png', scale=6, width=1080, height=1080)

# Plot slice plot and save
slice_plot = optuna.visualization.plot_slice(study)
slice_plot.write_image('es_optuna_hb/slice_plot.png', scale=6, width=1080, height=1080)

# Plot parallel coordinates plot and save
parallel_coords_plot = optuna.visualization.plot_parallel_coordinate(study)
parallel_coords_plot.write_image('es_optuna_hb/parallel_coordinates.png', scale=6, width=1080, height=1080)

# Plot optimization landscape and save
contour_plot = optuna.visualization.plot_contour(study)
contour_plot.write_image('es_optuna_hb/contour_plot.png', scale=6, width=1080, height=1080)

timeline_plot = optuna.visualization.plot_timeline(study)
timeline_plot.write_image('es_optuna_hb/timeline_plot.png', scale=6, width=1080, height=1080)

intermediate_values_plot = optuna.visualization.plot_intermediate_values(study)
intermediate_values_plot.write_image('es_optuna_hb/intermediate_values_plot.png', scale=6, width=1080, height=1080)
