# AI Malware Detector - Master's Thesis Project

## Overview

Welcome to the AI Malware Detector, a project developed as part of a Master's thesis. This project employs artificial intelligence techniques to detect malicious behavior in executable files. The process involves disassembly, word embedding training, and a comprehensive evaluation of the model's performance.

## Table of Contents
1. [Data Source](#data-source)
2. [Usage](#usage)
    1. [Disassembly](#disassembly)
    2. [Word Embedding Training](#word-embedding-training)
    3. [Feature Extraction](#feature-extraction)
    4. [Tune, Training, and Evaluation](#tunning-training-and-evaluation)
3. [Dependencies](#dependencies)
4. [Installation](#installation)

## Data Source<a name="data-source"></a>

The dataset used in this project is sourced from [VirusShare](https://virusshare.com/), a collaborative platform that shares malware samples to aid in cybersecurity research. The inclusion of VirusShare data enhances the diversity and realism of the dataset, contributing to the effectiveness of the AI-based malware detection model.


## Usage<a name="usage"></a>

This code is designed to run on the Polytechnic University of Catalonia's cluster, SERT. The program is divided into several steps, each of which can be executed independently. To run the entire program, follow these steps:

1. **Connect to the Cluster:**

    Connect to the [SERT](https://www.ac.upc.edu/ca/nosaltres/serveis-tic/blog/nou-node-amb-gpus-al-cluster-sert) cluster using SSH:

    ```bash
    ssh -X <user-name>@sert.ac.upc.edu
    ```

2. **Clone the Repository:**

    Clone the Git repository containing the code:

    ```bash
    git clone --recursive git@github.com:agirones/AI-Malware-Detector.git
    cd AI-Malware-Detector
    ```

3. **Install Dependencies:**

    Before installing the required dependencies, consider creating a virtual environment for your project. If you're unfamiliar with virtual environments or need guidance, you can follow the instructions provided [here](https://www.ac.upc.edu/app/wiki/serveis-tic/PreguntesFrequents/ModulsPython).

    Once your virtual environment is set up, install the dependencies by running the following command:

    ```bash
    python -m pip install -r requirements.txt
    ```


4. **Schedule and Monitor a Job:**

    After installing dependencies, you can schedule and monitor the job using the provided script:

    ```bash
    sbatch --mem=32G -A gpu -p gpu -q small_gpu --gres=gpu:1 --chdir="$(git rev-parse --show-toplevel)" code/jobs/run_all_process-job.sh
    watch -n 2 scontrol show jobid -dd <job_id>
    ```

    Replace `<job_id>` with the actual job ID generated when you submit the job.

    Additionally, once the job is running, you can find the output and error logs in the `code/jobs/logs` directory.


5. **Run Each Step Independently:**

    If you prefer to run specific steps independently, refer to the instructions provided in the subsequent sections of this README.

### Disassembly<a name="disassembly"></a>

The disassembly phase extracts instructions from executable files, generating a CSV file with labels indicating benign or malicious status. `objdump` and `sed` are used for text processing. The resulting `dataset.csv` file is foundational for subsequent stages.

To perform disassembly, execute:
```bash
python -m code.disassembly.disassembly
```

### Word Embedding Training<a name="word-embedding-training"></a>

The Word Embedding Training phase utilizes the Gensim Word2Vec model to train word embeddings with the data allocated in the `dataset.csv` file. This process involves tokenizing, indexing, and transforming instruction sequences into meaningful embeddings. After the training, the resulting word embedding model is stored in `word2vec.model` for future use.

To train word embeddings, use:
```bash
sbatch --mem=32G -A gpu -p gpu -q small_gpu --gres=gpu:1 --chdir="$(git rev-parse --show-toplevel)" code/jobs/job-word_embedding_train.sh
```

### Feature Extraction<a name="feature-extraction"></a>

The `feature_extraction.py` script loads the trained `word2vec.model` word embedding and computes feature vectors for each sample. Each of the feature vectors is embedded, providing a more meaningful representation of the data. These embedded feature vectors are then stored in three different files: `precomputed_embeddings_train_data.plk`, `precomputed_embeddings_validation_data.plk`, and `precomputed_embeddings_test_data.plk`.

To perform feature extraction, use:
```bash
sbatch --mem=32G -A gpu -p gpu -q small_gpu --gres=gpu:1 --chdir="$(git rev-parse --show-toplevel)" code/jobs/job-feature_extraction.sh
```

Ensure that the word embedding model is trained before running this script.


### Tuning, Training, and Evaluation<a name="tuning-training-and-evaluation"></a>

The `es_optuna_hb.py` script orchestrates the HyperBand Hyperparameter optimization process. This involves searching for the optimal hyperparameters to enhance the model's performance. The study, along with its graphs, is stored for reference.

To initiate hyperparameter tuning, training, and evaluation, run the following command:
```bash
sbatch --mem=32G -A gpu -p gpu -q small_gpu --gres=gpu:1 --chdir="$(git rev-parse --show-toplevel)" code/jobs/general_es_optuna_hb-job.sh
```

The script will perform HyperBand optimization, store the study details, and generate graphs showcasing the optimization process. After obtaining the optimal hyperparameters, the model is trained and evaluated, and the evaluation graphs are also saved.

As the results indicate that Bi-LSTM layers outperform LSTM layers, you can further refine the search space by tuning the model exclusively with Bi-LSTM layers. To do this, use the following command:

```bash
sbatch --mem=32G -A gpu -p gpu -q small_gpu --gres=gpu:1 --chdir="$(git rev-parse --show-toplevel)" code/jobs/bilstm_es_optuna_hb-job.sh
```

This script focuses on optimizing hyperparameters specifically for Bi-LSTM layers, leveraging the observed superior performance. The study and evaluation graphs will be stored accordingly.

Ensure that all preceding steps, including disassembly, word embedding training, and feature extraction, are completed before running these scripts. Tailoring the tuning process to the more effective Bi-LSTM layers can lead to improved results and a more efficient search space exploration.

## Dependencies<a name="dependencies"></a>

### Programming Language

- [Python](https://www.python.org/) (version 3.8.18)

### Machine Learning Libraries

- [tensorflow](https://www.tensorflow.org/) (version 2.11.1)
- [scikit-learn](https://scikit-learn.org/) (version 1.3.2)
- [gensim](https://radimrehurek.com/gensim/) (version 4.3.2)
- [optuna](https://optuna.org/) (version 3.4.0)

### Machine Learning Components

- [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) from gensim
- [Bidirectional, LSTM, Dense, Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers) layers from tensorflow.keras.layers
- [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) from tensorflow.keras.preprocessing.text
- [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) from tensorflow.keras.callbacks
- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn.model_selection
- [accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) metrics from sklearn.metrics

### General-Purpose Libraries

- [pandas](https://pandas.pydata.org/) (version 2.0.3)
- [matplotlib](https://matplotlib.org/) (version 3.7.3)
- [joblib](https://joblib.readthedocs.io/) (version 1.3.2)

## Installation<a name="installation"></a>

1. Clone the repository:

    ```bash
    git clone --recursive git@github.com:agirones/AI-Malware-Detector.git
    cd AI-Malware-Detector
    ```

2. Install the dependencies:

    ```bash
    python -m pip install -r requirements.txt
    ```

