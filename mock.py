import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd

# Example data: sequences of strings (instruction names)
train_data = pd.DataFrame({
    'Sequence': [
        "mov add sub jmp",
        "jmp add mov",
        "sub jmp",
        "mov sub jmp jmp jne"
    ],
    'Label': [0, 1, 0, 1]  # 0: Negative, 1: Positive
})

test_data = pd.DataFrame({
    'Sequence': [
        "mov jmp",
        "add"
    ],
    'Label': [1, 0]  # 1: Positive, 0: Negative
})

# Combine train and test data for vocabulary creation
all_sequences = train_data['Sequence'].tolist() + test_data['Sequence'].tolist()

# Tokenization and vocabulary creation
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_sequences)

# Tokenize and pad sequences for train and test data
train_sequences_tokenized = tokenizer.texts_to_sequences(train_data['Sequence'])
test_sequences_tokenized = tokenizer.texts_to_sequences(test_data['Sequence'])

def train_generator():
    for sequence, label in zip(train_sequences_tokenized, train_data['Label']):
        yield sequence, label

def test_generator():
    for sequence, label in zip(test_sequences_tokenized, test_data['Label']):
        yield sequence, label

# Create tf.data.Dataset from RaggedTensors
train_dataset = tf.data.Dataset.from_generator(
        train_generator,
        output_signature=(
            tf.TensorSpec(shape=(None,), dtype=tf.float16),
            tf.TensorSpec(shape=(), dtype=tf.float16)
            )
    )
test_dataset = tf.data.Dataset.from_generator(
        test_generator,
        output_signature=(
            tf.TensorSpec(shape=(None,), dtype=tf.float16),
            tf.TensorSpec(shape=(), dtype=tf.float16)
            )
    )

batch_size = 2
train_dataset = train_dataset.shuffle(3)
train_dataset = train_dataset.padded_batch(batch_size)
test_dataset = test_dataset.padded_batch(batch_size)

# Build the LSTM model
model = Sequential()
model.add(LSTM(32, input_shape=(None, 1)))  # Dynamic sequence length
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model using tf.data.Dataset
model.fit(train_dataset, epochs=10, verbose=1)

# Evaluate the model using tf.data.Dataset
test_loss, test_accuracy = model.evaluate(test_dataset)
print("Test loss:", test_loss)
print("Test accuracy:", test_accuracy)
