import numpy as np
import math
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
tf.config.run_functions_eagerly(True)


# Example data: sequences of strings (instruction names)
train_data = pd.DataFrame({
    'Sequence': [
        "mov add sub jmp",
        "jmp add mov",
        "sub jmp"
    ],
    'Label': [0, 1, 0]  # 0: Negative, 1: Positive
})

test_data = pd.DataFrame({
    'Sequence': [
        "mov jmp",
        "add"
    ],
    'Label': [1, 0]  # 1: Positive, 0: Negative
})

# Generator function that preprocesses and pads sequences on-the-fly
def data_generator(data, tokenizer, max_sequence_length, batch_size):
    num_samples = len(data)
    steps_per_epoch = num_samples // batch_size
    while True:
        for i in range(steps_per_epoch):
            batch_data = data[i * batch_size: (i + 1) * batch_size]
            batch_sequences = tokenizer.texts_to_sequences(batch_data['Sequence'])
            batch_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(
                batch_sequences, maxlen=max_sequence_length, padding='post'
            )
            batch_labels = batch_data['Label']
            yield batch_sequences_padded, batch_labels

# Create Tokenizer and calculate max sequence length
all_sequences = train_data['Sequence'].tolist() + test_data['Sequence'].tolist()
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_sequences)
max_sequence_length = max(len(seq.split()) for seq in all_sequences)

batch_size = 32
train_generator = data_generator(train_data, tokenizer, max_sequence_length, batch_size)
test_generator = data_generator(test_data, tokenizer, max_sequence_length, batch_size)

# Create tf.data.Dataset from generator functions
train_dataset = tf.data.Dataset.from_generator(
    lambda: train_generator, output_signature=(
        tf.TensorSpec(shape=(None, max_sequence_length), dtype=tf.int32),
        tf.TensorSpec(shape=(None,), dtype=tf.int32)
    )
)

test_dataset = tf.data.Dataset.from_generator(
    lambda: test_generator, output_signature=(
        tf.TensorSpec(shape=(None, max_sequence_length), dtype=tf.int32),
        tf.TensorSpec(shape=(None,), dtype=tf.int32)
    )
)

# Build the LSTM model
model = Sequential()
model.add(LSTM(32, input_shape=(None, max_sequence_length), activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

steps_per_epoch = math.ceil(len(train_data) / batch_size)


# Train the model using tf.data.Dataset
model.fit(train_dataset, epochs=10, steps_per_epoch=steps_per_epoch, verbose=1)

# Evaluate the model using tf.data.Dataset
test_loss, test_accuracy = model.evaluate(test_dataset, steps=len(test_data) // batch_size)
print("Test loss:", test_loss)
print("Test accuracy:", test_accuracy)
